# 10장 다중회귀 모형에서 가설검정

## 예제 10.1 지역별 이혼율


```python
import pandas as pd
from numpy import log # log instead of np.log
import statsmodels.formula.api as smf

Regko = pd.read_csv('csv/loedata/Regko.csv')
fm = 'log(divorce)~log(regpop)+log(drink)+log(hdrink) + log(smoke)+log(grdp/regpop) + log(aged)'
smf.ols(fm, data=Regko[Regko.type==2]).fit().summary()
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>      <td>log(divorce)</td>   <th>  R-squared:         </th> <td>   0.485</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.444</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   11.76</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 23 Jun 2024</td> <th>  Prob (F-statistic):</th> <td>3.00e-09</td>
</tr>
<tr>
  <th>Time:</th>                 <td>23:51:35</td>     <th>  Log-Likelihood:    </th> <td>  75.101</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    82</td>      <th>  AIC:               </th> <td>  -136.2</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    75</td>      <th>  BIC:               </th> <td>  -119.4</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>          <td>    0.1727</td> <td>    0.990</td> <td>    0.175</td> <td> 0.862</td> <td>   -1.799</td> <td>    2.144</td>
</tr>
<tr>
  <th>log(regpop)</th>        <td>   -0.0126</td> <td>    0.026</td> <td>   -0.487</td> <td> 0.628</td> <td>   -0.064</td> <td>    0.039</td>
</tr>
<tr>
  <th>log(drink)</th>         <td>   -0.0731</td> <td>    0.195</td> <td>   -0.374</td> <td> 0.709</td> <td>   -0.463</td> <td>    0.316</td>
</tr>
<tr>
  <th>log(hdrink)</th>        <td>    0.2494</td> <td>    0.075</td> <td>    3.321</td> <td> 0.001</td> <td>    0.100</td> <td>    0.399</td>
</tr>
<tr>
  <th>log(smoke)</th>         <td>    0.1834</td> <td>    0.160</td> <td>    1.148</td> <td> 0.255</td> <td>   -0.135</td> <td>    0.502</td>
</tr>
<tr>
  <th>log(grdp / regpop)</th> <td>    0.0953</td> <td>    0.040</td> <td>    2.404</td> <td> 0.019</td> <td>    0.016</td> <td>    0.174</td>
</tr>
<tr>
  <th>log(aged)</th>          <td>   -0.1969</td> <td>    0.062</td> <td>   -3.150</td> <td> 0.002</td> <td>   -0.321</td> <td>   -0.072</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.191</td> <th>  Durbin-Watson:     </th> <td>   1.805</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.909</td> <th>  Jarque-Bera (JB):  </th> <td>   0.335</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.097</td> <th>  Prob(JB):          </th> <td>   0.846</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.754</td> <th>  Cond. No.          </th> <td>1.18e+03</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.18e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
</div>


주석 \[2\]는 주의할 필요가 없는 정보이다.

## 예제 10.2 2년제 대학과 4년제 대학


```python
import pandas as pd
import numpy as np

twoyear = pd.read_csv('csv/wooldridge/twoyear.csv')
print(len(twoyear))
twoyear.head()
```

    6763





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>female</th>
      <th>phsrank</th>
      <th>BA</th>
      <th>AA</th>
      <th>black</th>
      <th>hispanic</th>
      <th>id</th>
      <th>exper</th>
      <th>jc</th>
      <th>univ</th>
      <th>...</th>
      <th>medcity</th>
      <th>submed</th>
      <th>lgcity</th>
      <th>sublg</th>
      <th>vlgcity</th>
      <th>subvlg</th>
      <th>ne</th>
      <th>nc</th>
      <th>south</th>
      <th>totcoll</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>65</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>19</td>
      <td>161</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>97</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>93</td>
      <td>119</td>
      <td>0.000000</td>
      <td>7.033333</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>7.033333</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>44</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>96</td>
      <td>81</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>34</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>119</td>
      <td>39</td>
      <td>0.266667</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.266667</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>80</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>132</td>
      <td>141</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 23 columns</p>
</div>




```python
reg0 = smf.ols('lwage~jc+univ+exper', data=twoyear).fit()
reg0.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>lwage</td>      <th>  R-squared:         </th> <td>   0.222</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.222</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   644.5</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 23 Jun 2024</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>23:51:35</td>     <th>  Log-Likelihood:    </th> <td> -3888.7</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  6763</td>      <th>  AIC:               </th> <td>   7785.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  6759</td>      <th>  BIC:               </th> <td>   7813.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    1.4723</td> <td>    0.021</td> <td>   69.910</td> <td> 0.000</td> <td>    1.431</td> <td>    1.514</td>
</tr>
<tr>
  <th>jc</th>        <td>    0.0667</td> <td>    0.007</td> <td>    9.767</td> <td> 0.000</td> <td>    0.053</td> <td>    0.080</td>
</tr>
<tr>
  <th>univ</th>      <td>    0.0769</td> <td>    0.002</td> <td>   33.298</td> <td> 0.000</td> <td>    0.072</td> <td>    0.081</td>
</tr>
<tr>
  <th>exper</th>     <td>    0.0049</td> <td>    0.000</td> <td>   31.397</td> <td> 0.000</td> <td>    0.005</td> <td>    0.005</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>81.514</td> <th>  Durbin-Watson:     </th> <td>   1.968</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 142.465</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.036</td> <th>  Prob(JB):          </th> <td>1.16e-31</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.707</td> <th>  Cond. No.          </th> <td>    512.</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>



```python
twoyear['totcoll'] = twoyear.jc + twoyear.univ
smf.ols('lwage~jc+totcoll+exper', data=twoyear).fit().summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>lwage</td>      <th>  R-squared:         </th> <td>   0.222</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.222</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   644.5</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 23 Jun 2024</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>23:51:35</td>     <th>  Log-Likelihood:    </th> <td> -3888.7</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  6763</td>      <th>  AIC:               </th> <td>   7785.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  6759</td>      <th>  BIC:               </th> <td>   7813.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    1.4723</td> <td>    0.021</td> <td>   69.910</td> <td> 0.000</td> <td>    1.431</td> <td>    1.514</td>
</tr>
<tr>
  <th>jc</th>        <td>   -0.0102</td> <td>    0.007</td> <td>   -1.468</td> <td> 0.142</td> <td>   -0.024</td> <td>    0.003</td>
</tr>
<tr>
  <th>totcoll</th>   <td>    0.0769</td> <td>    0.002</td> <td>   33.298</td> <td> 0.000</td> <td>    0.072</td> <td>    0.081</td>
</tr>
<tr>
  <th>exper</th>     <td>    0.0049</td> <td>    0.000</td> <td>   31.397</td> <td> 0.000</td> <td>    0.005</td> <td>    0.005</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>81.514</td> <th>  Durbin-Watson:     </th> <td>   1.968</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 142.465</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.036</td> <th>  Prob(JB):          </th> <td>1.16e-31</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.707</td> <th>  Cond. No.          </th> <td>    511.</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>


다음은 오차분산 추정치($s^2$) 부분을 제외한 분산 및 공분산 행렬이다.


```python
reg0.normalized_cov_params
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Intercept</th>
      <th>jc</th>
      <th>univ</th>
      <th>exper</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>0.002397</td>
      <td>-9.412177e-05</td>
      <td>-8.504379e-05</td>
      <td>-1.678074e-05</td>
    </tr>
    <tr>
      <th>jc</th>
      <td>-0.000094</td>
      <td>2.520413e-04</td>
      <td>1.042017e-05</td>
      <td>-9.287133e-08</td>
    </tr>
    <tr>
      <th>univ</th>
      <td>-0.000085</td>
      <td>1.042017e-05</td>
      <td>2.880909e-05</td>
      <td>2.125993e-07</td>
    </tr>
    <tr>
      <th>exper</th>
      <td>-0.000017</td>
      <td>-9.287133e-08</td>
      <td>2.125993e-07</td>
      <td>1.340290e-07</td>
    </tr>
  </tbody>
</table>
</div>



위에 $s^2$을 곱하면 분산 및 공분산들을 얻는다. $s^2$은 다음과 같다.


```python
s2 = reg0.scale # bad naming; this is s^2, not s.
s2
```




    0.1850190142404034




```python
ixx = reg0.normalized_cov_params # inv(X'X)
se = np.sqrt(s2*(ixx.jc.jc+ixx.univ.univ-2*ixx.jc.univ))
se
```




    0.006935906572922917




```python
bhat = reg0.params
tstat = (bhat.jc-bhat.univ)/se
tstat
```




    -1.4676566667599882




```python
from scipy.stats import t

pval = 2*(1-t.cdf(abs(tstat), reg0.df_resid))
pval
```




    0.14224403710482614



## 예제 10.3 2년제 대학과 4년제 대학 예제에서 F검정


```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

twoyear = pd.read_csv('csv/wooldridge/twoyear.csv')
reg1 = smf.ols('lwage~jc+univ+exper', data=twoyear).fit()
reg0 = smf.ols('lwage~I(jc+univ)+exper', data=twoyear).fit()
```


```python
Fstat = ((reg0.ssr-reg1.ssr)/1)/reg1.scale
Fstat
```




    2.154016091484592




```python
from scipy.stats import f

pval = 1-f.cdf(Fstat,1,reg1.df_resid)
pval
```




    0.14224403710559586




```python
# 동일한 검정을 간단히
reg1.f_test('jc=univ')
```




    <class 'statsmodels.stats.contrast.ContrastResults'>
    <F test: F=2.1540160914850386, p=0.14224403710559885, df_denom=6.76e+03, df_num=1>




```python
# 동일한 검정을 다른 식으로
twoyear['totcoll'] = twoyear.jc+twoyear.univ
ols1 = smf.ols('lwage~jc+totcoll+exper', data=twoyear).fit()
ols1.f_test('jc=0')
```




    <class 'statsmodels.stats.contrast.ContrastResults'>
    <F test: F=2.15401609148438, p=0.14224403710559885, df_denom=6.76e+03, df_num=1>



## 예제 10.4 병원 방문


```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

Doctor = pd.read_csv('csv/Ecdat/Doctor.csv')
ols1 = smf.ols('doctor~children+access+health', data=Doctor).fit()
ols1.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>doctor</td>      <th>  R-squared:         </th> <td>   0.092</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.086</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   16.15</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 23 Jun 2024</td> <th>  Prob (F-statistic):</th> <td>5.12e-10</td>
</tr>
<tr>
  <th>Time:</th>                 <td>23:51:35</td>     <th>  Log-Likelihood:    </th> <td> -1250.3</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   485</td>      <th>  AIC:               </th> <td>   2509.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   481</td>      <th>  BIC:               </th> <td>   2525.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    1.6075</td> <td>    0.418</td> <td>    3.843</td> <td> 0.000</td> <td>    0.786</td> <td>    2.429</td>
</tr>
<tr>
  <th>children</th>  <td>   -0.2512</td> <td>    0.110</td> <td>   -2.278</td> <td> 0.023</td> <td>   -0.468</td> <td>   -0.035</td>
</tr>
<tr>
  <th>access</th>    <td>    1.4995</td> <td>    0.783</td> <td>    1.915</td> <td> 0.056</td> <td>   -0.039</td> <td>    3.039</td>
</tr>
<tr>
  <th>health</th>    <td>    0.6506</td> <td>    0.102</td> <td>    6.399</td> <td> 0.000</td> <td>    0.451</td> <td>    0.850</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>660.517</td> <th>  Durbin-Watson:     </th>  <td>   1.879</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>131192.869</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 6.773</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>      <td>82.426</td>  <th>  Cond. No.          </th>  <td>    16.2</td> 
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>



```python
ols1.f_test('access=0')
```




    <class 'statsmodels.stats.contrast.ContrastResults'>
    <F test: F=3.6653550665625207, p=0.05614775007156983, df_denom=481, df_num=1>



## 예제 10.5 복잡한 제약들의 F검정


```python
# Continue with the 'twoyear' data
ols1 = smf.ols('lwage~jc+univ+exper', data=twoyear).fit() # unrestricted regression
ols1.f_test('jc=univ, univ=exper')
```




    <class 'statsmodels.stats.contrast.ContrastResults'>
    <F test: F=503.9720618690579, p=9.710503129065978e-205, df_denom=6.76e+03, df_num=2>



## 예제 10.6 라그랑지 승수(LM) 검정


```python
# Continue with the 'twoyear' data
# Two constraints: jc=univ=exper
ols0 = smf.ols('lwage~I(jc+univ+exper)', data=twoyear).fit() # restricted regression
twoyear['resid'] = ols0.resid
aux = smf.ols('resid~jc+univ+exper', data=twoyear).fit() # unrestricted regression
lmstat = ols0.nobs*aux.rsquared
lmstat
```




    877.6587033769132




```python
from scipy.stats import chi2

chi2.ppf(.95,2)
```




    5.991464547107979




```python
1-chi2.cdf(lmstat,2)
```




    0.0



# 13장 이분산

## 예제 13.3 이분산에 견고한 표준오차


```python
import pandas as pd
from numpy import log
import statsmodels.formula.api as smf

Housing = pd.read_csv('csv/Ecdat/Housing.csv')
#https://stackoverflow.com/questions/30553838/
#getting-statsmodels-to-use-heteroskedasticity-corrected-standard-errors-in-coeff
model = smf.ols('log(price)~log(lotsize)+bedrooms+bathrms', data=Housing)
ols = model.fit(cov_type = 'HC0')
ols.summary(slim=True)
```


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>log(price)</td> <th>  R-squared:         </th> <td>   0.504</td>
</tr>
<tr>
  <th>Model:</th>                <td>OLS</td>    <th>  Adj. R-squared:    </th> <td>   0.501</td>
</tr>
<tr>
  <th>No. Observations:</th>   <td>   546</td>   <th>  F-statistic:       </th> <td>   181.4</td>
</tr>
<tr>
  <th>Covariance Type:</th>      <td>HC0</td>    <th>  Prob (F-statistic):</th> <td>2.00e-81</td>
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>    6.6222</td> <td>    0.248</td> <td>   26.708</td> <td> 0.000</td> <td>    6.136</td> <td>    7.108</td>
</tr>
<tr>
  <th>log(lotsize)</th> <td>    0.4568</td> <td>    0.030</td> <td>   15.172</td> <td> 0.000</td> <td>    0.398</td> <td>    0.516</td>
</tr>
<tr>
  <th>bedrooms</th>     <td>    0.0892</td> <td>    0.018</td> <td>    4.999</td> <td> 0.000</td> <td>    0.054</td> <td>    0.124</td>
</tr>
<tr>
  <th>bathrms</th>      <td>    0.2368</td> <td>    0.026</td> <td>    8.971</td> <td> 0.000</td> <td>    0.185</td> <td>    0.288</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC0)
</div>


## 예제 13.4 통상적인 표준오차와 여러 견고한 표준오차들의 비교


```python
ols = model.fit() # fit again (ordinary se)
pd.DataFrame({'ord': ols.bse, 'hc0': ols.HC0_se, 'hc1': ols.HC1_se, 'hc2': ols.HC2_se, 'hc3': ols.HC3_se})
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ord</th>
      <th>hc0</th>
      <th>hc1</th>
      <th>hc2</th>
      <th>hc3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>0.241234</td>
      <td>0.247946</td>
      <td>0.248859</td>
      <td>0.249356</td>
      <td>0.250780</td>
    </tr>
    <tr>
      <th>log(lotsize)</th>
      <td>0.028983</td>
      <td>0.030110</td>
      <td>0.030221</td>
      <td>0.030286</td>
      <td>0.030464</td>
    </tr>
    <tr>
      <th>bedrooms</th>
      <td>0.016513</td>
      <td>0.017850</td>
      <td>0.017916</td>
      <td>0.017985</td>
      <td>0.018122</td>
    </tr>
    <tr>
      <th>bathrms</th>
      <td>0.024479</td>
      <td>0.026392</td>
      <td>0.026490</td>
      <td>0.026611</td>
      <td>0.026834</td>
    </tr>
  </tbody>
</table>
</div>



## 예제 13.5 지역별 사망률


```python
import pandas as pd
import statsmodels.formula.api as smf

Death = pd.read_csv('csv/loedata/Death.csv')
model = smf.ols('deathrate~drink+smoke+aged+vehipc+C(year)', data=Death) # C=categorical
model.fit().summary(slim=True)
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>deathrate</td> <th>  R-squared:         </th> <td>   0.921</td> 
</tr>
<tr>
  <th>Model:</th>               <td>OLS</td>    <th>  Adj. R-squared:    </th> <td>   0.919</td> 
</tr>
<tr>
  <th>No. Observations:</th>  <td>   258</td>   <th>  F-statistic:       </th> <td>   487.3</td> 
</tr>
<tr>
  <th>Covariance Type:</th>  <td>nonrobust</td> <th>  Prob (F-statistic):</th> <td>3.42e-135</td>
</tr>
</table>
<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>   -0.2241</td> <td>    0.769</td> <td>   -0.291</td> <td> 0.771</td> <td>   -1.739</td> <td>    1.291</td>
</tr>
<tr>
  <th>C(year)[T.2009]</th> <td>   -0.3788</td> <td>    0.098</td> <td>   -3.867</td> <td> 0.000</td> <td>   -0.572</td> <td>   -0.186</td>
</tr>
<tr>
  <th>C(year)[T.2010]</th> <td>   -0.3510</td> <td>    0.102</td> <td>   -3.457</td> <td> 0.001</td> <td>   -0.551</td> <td>   -0.151</td>
</tr>
<tr>
  <th>drink</th>           <td>    0.0064</td> <td>    0.011</td> <td>    0.594</td> <td> 0.553</td> <td>   -0.015</td> <td>    0.028</td>
</tr>
<tr>
  <th>smoke</th>           <td>    0.0333</td> <td>    0.018</td> <td>    1.873</td> <td> 0.062</td> <td>   -0.002</td> <td>    0.068</td>
</tr>
<tr>
  <th>aged</th>            <td>    0.4027</td> <td>    0.010</td> <td>   38.401</td> <td> 0.000</td> <td>    0.382</td> <td>    0.423</td>
</tr>
<tr>
  <th>vehipc</th>          <td>    1.4079</td> <td>    1.163</td> <td>    1.211</td> <td> 0.227</td> <td>   -0.882</td> <td>    3.698</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.91e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
</div>


위의 주석 \[2\]는 [condition number](https://en.wikipedia.org/wiki/Condition_number)에 관한 것이다. 이 값은 $X'X$의 가장 큰 eigenvalue와 가장 작은 eigenvalue 간 비율에 제곱근을 취한 것과 동일하다. 근사적인 다중공선성의 지표가 되기도 하는데, 크게 신경 쓰지 않아도 된다.


```python
model.fit(cov_type='HC3').summary(slim=True)
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>deathrate</td> <th>  R-squared:         </th> <td>   0.921</td> 
</tr>
<tr>
  <th>Model:</th>               <td>OLS</td>    <th>  Adj. R-squared:    </th> <td>   0.919</td> 
</tr>
<tr>
  <th>No. Observations:</th>  <td>   258</td>   <th>  F-statistic:       </th> <td>   650.1</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>HC3</td>    <th>  Prob (F-statistic):</th> <td>8.45e-150</td>
</tr>
</table>
<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>   -0.2241</td> <td>    0.785</td> <td>   -0.285</td> <td> 0.775</td> <td>   -1.763</td> <td>    1.315</td>
</tr>
<tr>
  <th>C(year)[T.2009]</th> <td>   -0.3788</td> <td>    0.095</td> <td>   -3.996</td> <td> 0.000</td> <td>   -0.565</td> <td>   -0.193</td>
</tr>
<tr>
  <th>C(year)[T.2010]</th> <td>   -0.3510</td> <td>    0.104</td> <td>   -3.371</td> <td> 0.001</td> <td>   -0.555</td> <td>   -0.147</td>
</tr>
<tr>
  <th>drink</th>           <td>    0.0064</td> <td>    0.011</td> <td>    0.565</td> <td> 0.572</td> <td>   -0.016</td> <td>    0.029</td>
</tr>
<tr>
  <th>smoke</th>           <td>    0.0333</td> <td>    0.019</td> <td>    1.770</td> <td> 0.077</td> <td>   -0.004</td> <td>    0.070</td>
</tr>
<tr>
  <th>aged</th>            <td>    0.4027</td> <td>    0.010</td> <td>   39.371</td> <td> 0.000</td> <td>    0.383</td> <td>    0.423</td>
</tr>
<tr>
  <th>vehipc</th>          <td>    1.4079</td> <td>    1.295</td> <td>    1.087</td> <td> 0.277</td> <td>   -1.130</td> <td>    3.946</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC3)<br/>[2] The condition number is large, 1.91e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
</div>


## 예제 13.6 사망률 분석에서 F검정


```python
# Continue
hypo = 'drink=0, smoke=0'
rego = model.fit()
print(rego.f_test(hypo))
regh = model.fit(cov_type='HC3')
print(regh.f_test(hypo))
```

    <F test: F=3.2488401914796645, p=0.04045761180359891, df_denom=251, df_num=2>
    <F test: F=2.9862362091563925, p=0.052273714967270596, df_denom=251, df_num=2>


## 예제 13.8 사망률 모형의 WLS 추정


```python
import pandas as pd
import statsmodels.formula.api as smf

Death = pd.read_csv('csv/loedata/Death.csv')
fm = 'deathrate~drink+smoke+aged+vehipc+C(year)'
wls = smf.wls(fm, data=Death, weights=Death.regpop).fit()
wls.summary(slim=True)
```




<table class="simpletable">
<caption>WLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>deathrate</td> <th>  R-squared:         </th> <td>   0.941</td> 
</tr>
<tr>
  <th>Model:</th>               <td>WLS</td>    <th>  Adj. R-squared:    </th> <td>   0.939</td> 
</tr>
<tr>
  <th>No. Observations:</th>  <td>   258</td>   <th>  F-statistic:       </th> <td>   663.9</td> 
</tr>
<tr>
  <th>Covariance Type:</th>  <td>nonrobust</td> <th>  Prob (F-statistic):</th> <td>7.08e-151</td>
</tr>
</table>
<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>   -0.5815</td> <td>    0.764</td> <td>   -0.761</td> <td> 0.448</td> <td>   -2.087</td> <td>    0.924</td>
</tr>
<tr>
  <th>C(year)[T.2009]</th> <td>   -0.2963</td> <td>    0.096</td> <td>   -3.093</td> <td> 0.002</td> <td>   -0.485</td> <td>   -0.108</td>
</tr>
<tr>
  <th>C(year)[T.2010]</th> <td>   -0.2977</td> <td>    0.099</td> <td>   -3.008</td> <td> 0.003</td> <td>   -0.493</td> <td>   -0.103</td>
</tr>
<tr>
  <th>drink</th>           <td>    0.0166</td> <td>    0.011</td> <td>    1.575</td> <td> 0.117</td> <td>   -0.004</td> <td>    0.037</td>
</tr>
<tr>
  <th>smoke</th>           <td>    0.0327</td> <td>    0.018</td> <td>    1.830</td> <td> 0.068</td> <td>   -0.002</td> <td>    0.068</td>
</tr>
<tr>
  <th>aged</th>            <td>    0.4105</td> <td>    0.010</td> <td>   42.003</td> <td> 0.000</td> <td>    0.391</td> <td>    0.430</td>
</tr>
<tr>
  <th>vehipc</th>          <td>    0.5491</td> <td>    1.185</td> <td>    0.464</td> <td> 0.643</td> <td>   -1.784</td> <td>    2.882</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.02e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
</div>


## 예제 13.9 FGLS의 예


```python
import statsmodels.formula.api as smf
from numpy import log, exp

# Continue
fm = 'deathrate~drink+smoke+aged+vehipc+C(year)'
# Step 1
ols = smf.ols(fm, data=Death).fit()
Death['u'] = ols.resid
# Step 2
fm_aux = fm.replace('deathrate', 'log(u**2)')
aux = smf.ols(fm_aux, data=Death).fit()
# Step 3
h = exp(aux.fittedvalues)
# Step 4
fgls = smf.wls(fm, data=Death, weights=1/h).fit()
fgls.summary(slim=True)
```




<table class="simpletable">
<caption>WLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>deathrate</td> <th>  R-squared:         </th> <td>   0.936</td> 
</tr>
<tr>
  <th>Model:</th>               <td>WLS</td>    <th>  Adj. R-squared:    </th> <td>   0.934</td> 
</tr>
<tr>
  <th>No. Observations:</th>  <td>   258</td>   <th>  F-statistic:       </th> <td>   608.2</td> 
</tr>
<tr>
  <th>Covariance Type:</th>  <td>nonrobust</td> <th>  Prob (F-statistic):</th> <td>2.13e-146</td>
</tr>
</table>
<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>   -0.2457</td> <td>    0.773</td> <td>   -0.318</td> <td> 0.751</td> <td>   -1.769</td> <td>    1.278</td>
</tr>
<tr>
  <th>C(year)[T.2009]</th> <td>   -0.3521</td> <td>    0.090</td> <td>   -3.914</td> <td> 0.000</td> <td>   -0.529</td> <td>   -0.175</td>
</tr>
<tr>
  <th>C(year)[T.2010]</th> <td>   -0.2974</td> <td>    0.097</td> <td>   -3.056</td> <td> 0.002</td> <td>   -0.489</td> <td>   -0.106</td>
</tr>
<tr>
  <th>drink</th>           <td>    0.0087</td> <td>    0.011</td> <td>    0.817</td> <td> 0.414</td> <td>   -0.012</td> <td>    0.030</td>
</tr>
<tr>
  <th>smoke</th>           <td>    0.0363</td> <td>    0.017</td> <td>    2.187</td> <td> 0.030</td> <td>    0.004</td> <td>    0.069</td>
</tr>
<tr>
  <th>aged</th>            <td>    0.4050</td> <td>    0.010</td> <td>   40.215</td> <td> 0.000</td> <td>    0.385</td> <td>    0.425</td>
</tr>
<tr>
  <th>vehipc</th>          <td>    0.7946</td> <td>    1.113</td> <td>    0.714</td> <td> 0.476</td> <td>   -1.397</td> <td>    2.987</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.99e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
</div>


## 이분산 존재 검정


```python
import pandas as pd
import numpy as np

# Generate data
np.random.seed(101)
n = 50
x1 = np.random.normal(size=n)
x2 = np.random.normal(size=n)
u = [a*b for a,b in zip(x1,np.random.normal(size=n))]
y = 1+x1-x2+u
DF = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2})
```


```python
import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_breuschpagan as bptest

uhat = sm.OLS.from_formula('y~x1+x2', data=DF).fit().resid
bp = bptest(uhat, sm.add_constant(pd.DataFrame({"x1":x1, "x1sq":x1**2})))
bp # (lm, lm_pvalue, fvalue, f_pvalue)
```




    (25.43047687088538,
     3.00498363642047e-06,
     24.32347601234627,
     5.604385363360125e-08)



맨 앞 숫자가 LM 검정통계, 그 다음이 p값이다. 세 번째 숫자는 F 검정통계, 네 번째 숫자는 이에 해당하는 p값이다.


```python
DF['u2'] = uhat**2
aux = sm.OLS.from_formula('u2~x1+I(x1**2)', data=DF).fit()
aux.nobs*aux.rsquared
```




    25.43047687088538



LM 검정통계는 앞에서 구한 값과 동일하다.

# 6장 최소제곱법을 이용한 가설검정


```python
## https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html#scipy.stats.t
from scipy.stats import t
t.ppf(.975, 23)
```



    2.0686576104190406



```python
t.ppf(.95, 23)
```



    1.713871527747047


## Housing


```python
import numpy as np
import pandas as pd
import statsmodels.formula.api as smf

Housing = pd.read_csv('csv/Ecdat/Housing.csv')
ols = smf.ols('np.log(price)~np.log(lotsize)', data=Housing).fit()
ols.summary()
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>      <td>np.log(price)</td>  <th>  R-squared:         </th> <td>   0.336</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.335</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   275.8</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td> <th>  Prob (F-statistic):</th> <td>2.14e-50</td>
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:54</td>     <th>  Log-Likelihood:    </th> <td> -122.36</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   546</td>      <th>  AIC:               </th> <td>   248.7</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   544</td>      <th>  BIC:               </th> <td>   257.3</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>    6.4685</td> <td>    0.277</td> <td>   23.374</td> <td> 0.000</td> <td>    5.925</td> <td>    7.012</td>
</tr>
<tr>
  <th>np.log(lotsize)</th> <td>    0.5422</td> <td>    0.033</td> <td>   16.606</td> <td> 0.000</td> <td>    0.478</td> <td>    0.606</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.255</td> <th>  Durbin-Watson:     </th> <td>   1.086</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.880</td> <th>  Jarque-Bera (JB):  </th> <td>   0.333</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.045</td> <th>  Prob(JB):          </th> <td>   0.847</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.920</td> <th>  Cond. No.          </th> <td>    183.</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>


```python
from scipy.stats import t

t.ppf(.995, 544)
```

    2.5848970040670145


## 예제 6.1 나이와 통신비


```python
import pandas as pd
from statsmodels.api import OLS

Hcons = pd.read_csv('csv/loedata/Hcons.csv')
print(Hcons.describe())
```


```
               age         comm          rec
count  6723.000000  6723.000000  6723.000000
mean     45.860033     6.841078     5.162530
std       8.237180     3.925046     4.836962
min      30.000000     0.000000     0.000000
25%      39.000000     4.261053     2.253281
50%      46.000000     6.031846     3.856771
75%      53.000000     8.440650     6.514518
max      60.000000    37.129649    72.807483
```


```python
ols = smf.ols('comm~age', data=Hcons).fit()
ols.summary()
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>comm</td>       <th>  R-squared:         </th> <td>   0.001</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.001</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.522</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td> <th>  Prob (F-statistic):</th>  <td>0.0335</td>  
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:55</td>     <th>  Log-Likelihood:    </th> <td> -18730.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  6723</td>      <th>  AIC:               </th> <td>3.746e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  6721</td>      <th>  BIC:               </th> <td>3.748e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    6.2744</td> <td>    0.271</td> <td>   23.176</td> <td> 0.000</td> <td>    5.744</td> <td>    6.805</td>
</tr>
<tr>
  <th>age</th>       <td>    0.0124</td> <td>    0.006</td> <td>    2.127</td> <td> 0.033</td> <td>    0.001</td> <td>    0.024</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>2757.039</td> <th>  Durbin-Watson:     </th> <td>   1.844</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>15710.710</td>
</tr>
<tr>
  <th>Skew:</th>           <td> 1.887</td>  <th>  Prob(JB):          </th> <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>       <td> 9.468</td>  <th>  Cond. No.          </th> <td>    264.</td> 
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>


## p값


```python
from scipy.stats import t

2*t.cdf(-1.54,48)
```

```
0.13012747345659167
```



```python
2*(1-t.cdf(1.54,48))
```

```
0.1301274734565918
```

## t(544) 분포의 5% 임계값


```python
t.ppf(.975, 544)
```

```
1.9643343306673329
```


## 예제 6.2 삶의 만족도


```python
import pandas as pd
Klosa = pd.read_csv('csv/loedata/Klosa.csv')
# Subsetting
Klosa1 = Klosa[(Klosa['working']==0) & (Klosa['age']>=65)]

import statsmodels.formula.api as smf
fm = 'satisfy5~married'
smf.ols(fm, data=Klosa1).fit().summary()
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>satisfy5</td>     <th>  R-squared:         </th> <td>   0.026</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.025</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   28.49</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td> <th>  Prob (F-statistic):</th> <td>1.15e-07</td>
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:55</td>     <th>  Log-Likelihood:    </th> <td> -4641.8</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1060</td>      <th>  AIC:               </th> <td>   9288.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1058</td>      <th>  BIC:               </th> <td>   9297.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   51.5534</td> <td>    0.851</td> <td>   60.562</td> <td> 0.000</td> <td>   49.883</td> <td>   53.224</td>
</tr>
<tr>
  <th>married</th>   <td>    6.3365</td> <td>    1.187</td> <td>    5.337</td> <td> 0.000</td> <td>    4.007</td> <td>    8.666</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>27.276</td> <th>  Durbin-Watson:     </th> <td>   1.739</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  29.010</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.402</td> <th>  Prob(JB):          </th> <td>5.02e-07</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.904</td> <th>  Cond. No.          </th> <td>    2.65</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>



```python
import numpy as np
np.std(Klosa.satisfy5)
```



```
18.522274892616505
```



```python
smf.ols(fm, data=Klosa1[Klosa1.hlth3>=0]).fit().summary()
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>satisfy5</td>     <th>  R-squared:         </th> <td>   0.002</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.001</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.5670</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td> <th>  Prob (F-statistic):</th>  <td> 0.452</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:55</td>     <th>  Log-Likelihood:    </th> <td> -1250.9</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   300</td>      <th>  AIC:               </th> <td>   2506.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   298</td>      <th>  BIC:               </th> <td>   2513.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   62.1186</td> <td>    1.446</td> <td>   42.960</td> <td> 0.000</td> <td>   59.273</td> <td>   64.964</td>
</tr>
<tr>
  <th>married</th>   <td>    1.3978</td> <td>    1.856</td> <td>    0.753</td> <td> 0.452</td> <td>   -2.256</td> <td>    5.051</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>11.384</td> <th>  Durbin-Watson:     </th> <td>   1.656</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.003</td> <th>  Jarque-Bera (JB):  </th> <td>  12.106</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.484</td> <th>  Prob(JB):          </th> <td> 0.00235</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.824</td> <th>  Cond. No.          </th> <td>    2.95</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>



```python
print(smf.ols(fm, data=Klosa1[Klosa1.hlth3<0]).fit().summary())
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>satisfy5</td>     <th>  R-squared:         </th> <td>   0.028</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.026</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   21.54</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td> <th>  Prob (F-statistic):</th> <td>4.08e-06</td>
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:55</td>     <th>  Log-Likelihood:    </th> <td> -3344.5</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   760</td>      <th>  AIC:               </th> <td>   6693.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   758</td>      <th>  BIC:               </th> <td>   6702.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   48.4131</td> <td>    0.991</td> <td>   48.849</td> <td> 0.000</td> <td>   46.468</td> <td>   50.359</td>
</tr>
<tr>
  <th>married</th>   <td>    6.6558</td> <td>    1.434</td> <td>    4.641</td> <td> 0.000</td> <td>    3.841</td> <td>    9.471</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>11.637</td> <th>  Durbin-Watson:     </th> <td>   1.738</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.003</td> <th>  Jarque-Bera (JB):  </th> <td>  11.918</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.294</td> <th>  Prob(JB):          </th> <td> 0.00258</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.828</td> <th>  Cond. No.          </th> <td>    2.57</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>



## 신뢰구간


```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

Housing = pd.read_csv('csv/Ecdat/Housing.csv')
ols = smf.ols('np.log(price)~np.log(lotsize)', data=Housing).fit()

## https://stackoverflow.com/questions/44302099/python-statsmodels-ols-confidence-interval
## https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.conf_int.html
print(ols.conf_int(alpha=.05))
```

```
                        0         1
Intercept        5.924920  7.012143
np.log(lotsize)  0.478043  0.606315
```



```python
print(ols.conf_int(alpha=.01))
```

```
                        0         1
Intercept        5.753185  7.183879
np.log(lotsize)  0.457782  0.626576
```



## 예제 6.3 2012년 상장기업 급여


```python
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf

Ksalary = pd.read_csv('csv/loedata/Ksalary.csv')
Ksalary1 = Ksalary[(Ksalary.kospi==1) & (Ksalary.sector=='ElecElectron')]
ols = smf.ols('np.log(avgsal)~np.log(sales/emp)', data=Ksalary1).fit()
ols.summary()
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>     <td>np.log(avgsal)</td>  <th>  R-squared:         </th> <td>   0.101</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.084</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   6.072</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td> <th>  Prob (F-statistic):</th>  <td>0.0169</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:55</td>     <th>  Log-Likelihood:    </th> <td>-0.95089</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    56</td>      <th>  AIC:               </th> <td>   5.902</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    54</td>      <th>  BIC:               </th> <td>   9.952</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
           <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>           <td>    3.8047</td> <td>    0.046</td> <td>   82.889</td> <td> 0.000</td> <td>    3.713</td> <td>    3.897</td>
</tr>
<tr>
  <th>np.log(sales / emp)</th> <td>    0.1225</td> <td>    0.050</td> <td>    2.464</td> <td> 0.017</td> <td>    0.023</td> <td>    0.222</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.191</td> <th>  Durbin-Watson:     </th> <td>   0.216</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.909</td> <th>  Jarque-Bera (JB):  </th> <td>   0.337</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.119</td> <th>  Prob(JB):          </th> <td>   0.845</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.704</td> <th>  Cond. No.          </th> <td>    2.32</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>



```python
print(ols.conf_int(.01))
```

```
                            0         1
Intercept            3.682129  3.927238
np.log(sales / emp) -0.010231  0.255249
```



```python
print(ols.conf_int(.05))
```

```
                            0         1
Intercept            3.712658  3.896709
np.log(sales / emp)  0.022836  0.222183
```



## 예제 6.4 주택가격


```python
Housing['unitprice'] = [pi/li for pi,li in zip(Housing.price, Housing.lotsize)]
ols = smf.ols('np.log(unitprice)~np.log(lotsize)', data=Housing).fit()
ols.summary()
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>np.log(unitprice)</td> <th>  R-squared:         </th> <td>   0.265</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.264</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   196.6</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td>  <th>  Prob (F-statistic):</th> <td>2.36e-38</td>
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:55</td>      <th>  Log-Likelihood:    </th> <td> -122.36</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   546</td>       <th>  AIC:               </th> <td>   248.7</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   544</td>       <th>  BIC:               </th> <td>   257.3</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>       <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>       <td>    6.4685</td> <td>    0.277</td> <td>   23.374</td> <td> 0.000</td> <td>    5.925</td> <td>    7.012</td>
</tr>
<tr>
  <th>np.log(lotsize)</th> <td>   -0.4578</td> <td>    0.033</td> <td>  -14.022</td> <td> 0.000</td> <td>   -0.522</td> <td>   -0.394</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.255</td> <th>  Durbin-Watson:     </th> <td>   1.086</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.880</td> <th>  Jarque-Bera (JB):  </th> <td>   0.333</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.045</td> <th>  Prob(JB):          </th> <td>   0.847</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.920</td> <th>  Cond. No.          </th> <td>    183.</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>


## 예제 6.5 담배소비의 가격탄력성


```python
import pandas as pd
import statsmodels.formula.api as smf

data = pd.read_csv('csv/AER/CigarettesB.csv')
smf.ols('packs~price', data=data).fit().summary()
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>packs</td>      <th>  R-squared:         </th> <td>   0.291</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.275</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   18.08</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td> <th>  Prob (F-statistic):</th> <td>0.000108</td>
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:55</td>     <th>  Log-Likelihood:    </th> <td>  19.195</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    46</td>      <th>  AIC:               </th> <td>  -34.39</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>  -30.73</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    5.0941</td> <td>    0.063</td> <td>   81.247</td> <td> 0.000</td> <td>    4.968</td> <td>    5.220</td>
</tr>
<tr>
  <th>price</th>     <td>   -1.1983</td> <td>    0.282</td> <td>   -4.253</td> <td> 0.000</td> <td>   -1.766</td> <td>   -0.630</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.860</td> <th>  Durbin-Watson:     </th> <td>   2.307</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.395</td> <th>  Jarque-Bera (JB):  </th> <td>   1.209</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.389</td> <th>  Prob(JB):          </th> <td>   0.546</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.164</td> <th>  Cond. No.          </th> <td>    12.2</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>



```python
print(smf.ols('I(packs+price)~price', data=data).fit().summary())
```

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>    <td>I(packs + price)</td> <th>  R-squared:         </th> <td>   0.011</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.011</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.4953</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 11 Feb 2026</td> <th>  Prob (F-statistic):</th>  <td> 0.485</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>02:23:55</td>     <th>  Log-Likelihood:    </th> <td>  19.195</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    46</td>      <th>  AIC:               </th> <td>  -34.39</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>  -30.73</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    5.0941</td> <td>    0.063</td> <td>   81.247</td> <td> 0.000</td> <td>    4.968</td> <td>    5.220</td>
</tr>
<tr>
  <th>price</th>     <td>   -0.1983</td> <td>    0.282</td> <td>   -0.704</td> <td> 0.485</td> <td>   -0.766</td> <td>    0.370</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.860</td> <th>  Durbin-Watson:     </th> <td>   2.307</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.395</td> <th>  Jarque-Bera (JB):  </th> <td>   1.209</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.389</td> <th>  Prob(JB):          </th> <td>   0.546</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.164</td> <th>  Cond. No.          </th> <td>    12.2</td>
</tr>
</table>
<div class="reg-notes">
Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>

